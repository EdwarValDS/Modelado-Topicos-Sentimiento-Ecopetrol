{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes de datos de noticias\n",
    "\n",
    "1. La república\n",
    "2. Hydrocarbons Colombia\n",
    "3. Portafolio \n",
    "\n",
    "> Para descargar nuevos datos, solamente se debe ejecutar todo para actualizar la base de datos finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from dateutil import parser\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "meses_mapping = {\n",
    "    'ene': 'Jan',\n",
    "    'feb': 'Feb',\n",
    "    'mar': 'Mar',\n",
    "    'abr': 'Apr',\n",
    "    'may': 'May',\n",
    "    'jun': 'Jun',\n",
    "    'jul': 'Jul',\n",
    "    'ago': 'Aug',\n",
    "    'sep': 'Sep',\n",
    "    'oct': 'Oct',\n",
    "    'nov': 'Nov',\n",
    "    'dic': 'Dec'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_experimental_option(\"detach\",True)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.get(\"https://www.larepublica.co/buscar?term=ecopetrol\")\n",
    "#driver.maximize_window()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "max_duration_seconds = 10\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    while time.time() - start_time < max_duration_seconds:\n",
    "        # Encontrar el botón \"VER MÁS\"\n",
    "        ver_mas_button = driver.find_element(By.CLASS_NAME, \"btn.analisisSect\")\n",
    "        # Hacer scroll hasta el botón\n",
    "        ver_mas_button.location_once_scrolled_into_view\n",
    "        time.sleep(2)\n",
    "        ver_mas_button.click()\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo hacer clic en el botón 'VER MÁS': {e}\")\n",
    "finally:\n",
    "    html = driver.page_source\n",
    "    #with open(\"html_ec_republica.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    #    file.write(html)\n",
    "driver.quit()\n",
    "\n",
    "html = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "articles = html.find(\"div\", class_=\"result-list\")\n",
    "\n",
    "# Buscar todos los elementos <a> con la clase \"result\"\n",
    "articles = articles.find_all('a', class_='result')\n",
    "\n",
    "titles = []\n",
    "links = []\n",
    "dates = []\n",
    "# Iterar sobre los enlaces y extraer la información deseada\n",
    "for link in articles:\n",
    "    title = link.find('h3').text\n",
    "    url = link['href']\n",
    "    date = link.find('span', class_='date').text\n",
    "    # Almacenar la información en las listas\n",
    "    titles.append(title)\n",
    "    links.append(url)\n",
    "    dates.append(date)\n",
    "\n",
    "data1 = pd.DataFrame({\"Date\": dates, \"Title\": titles, \"Link\": links})\n",
    "\n",
    "data1[\"Link\"] = \"https://www.larepublica.co\" + data1[\"Link\"] \n",
    "\n",
    "texts = []\n",
    "leads = []\n",
    "\n",
    "for l in data1[\"Link\"]:\n",
    "    req = Request(url=l)\n",
    "    try: \n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response, \"html.parser\")\n",
    "        try:\n",
    "            lead = html.find(\"div\", class_=\"lead\").text\n",
    "            new = html.find(\"div\", class_=\"html-content\").text\n",
    "            lead = lead.replace('\\n', ' ')\n",
    "            new = new.replace('\\n', ' ')\n",
    "            text = lead + new\n",
    "            texts.append(text)\n",
    "            leads.append(lead)\n",
    "        except:\n",
    "            lead = \"na\"\n",
    "            new  = \"na\"\n",
    "            text = lead + new\n",
    "            texts.append(text)\n",
    "            leads.append(lead)\n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    time.sleep(1)\n",
    "\n",
    "data1[\"Headline\"] = leads\n",
    "data1[\"Article\"] = texts\n",
    "\n",
    "\n",
    "# Limpieza inicial\n",
    "data1['Article'] = data1['Article'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data1['Article'] = data1['Article'].str.replace(r'\\n', ' ')  \n",
    "data1['Article'] = data1['Article'].str.strip()  \n",
    "data1['Title'] = data1['Title'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data1['Title'] = data1['Title'].str.replace(r'\\n', ' ')  \n",
    "data1['Title'] = data1['Title'].str.strip()  \n",
    "data1['Headline'] = data1['Headline'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data1['Headline'] = data1['Headline'].str.replace(r'\\n', ' ')  \n",
    "data1['Headline'] = data1['Headline'].str.strip()  \n",
    "\n",
    "\n",
    "data1[\"Source\"] = \"La República\"\n",
    "\n",
    "data1['Date'] = data1['Date'].str.replace('.', '')\n",
    "\n",
    "data1['Date'] = data1['Date'].apply(lambda x: ' '.join([meses_mapping[mes] if mes in meses_mapping else mes for mes in x.split()]))\n",
    "data1['Date'] = pd.to_datetime(data1['Date'], format='%b %d, %Y', errors='coerce')\n",
    "\n",
    "# Definir las columnas que se van a traducir\n",
    "columns_to_translate = [\"Title\",\"Headline\"]\n",
    "\n",
    "for column in columns_to_translate:\n",
    "    try:\n",
    "      data1[column] = data1[column].apply(lambda x: GoogleTranslator(source='es', target='en').translate(x))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "#data1.to_csv(\"./LaRepublica.csv\", index=False)\n",
    "\n",
    "old_data1 = pd.read_csv(\"./Datos/Datos_analisis_EC_traducidos.csv\")\n",
    "old_data1 = old_data1[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "new_data1 = pd.concat([old_data1,data1], ignore_index=True).drop_duplicates(subset=[\"Link\"])\n",
    "new_data1 = new_data1[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "#new_data1[\"Date\"] = pd.to_datetime(new_data1[\"Date\"])\n",
    "#new_data1 = new_data1.sort_values(by=\"Date\")\n",
    "new_data1.to_csv(\"./Datos/Datos_analisis_EC_traducidos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hydrocarbons Colombia\n",
    "\n",
    "pagsrange = [str(numero) for numero in range(1, 4)]\n",
    "\n",
    "titles = []\n",
    "links = []\n",
    "dates = []\n",
    "texts = []\n",
    "\n",
    "for pag in pagsrange:\n",
    "    url = \"https://hydrocarbonscolombia.com/page/\" + pag + \"/?s=ECOPETROL&start_date&end_date&cate&usefulness\"\n",
    "    try:\n",
    "        req = Request(url=url)\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response, \"html.parser\")\n",
    "        articles = html.find_all(\"div\", class_=\"item\")\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                if 'active' in article['class']:\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "                title_element = article.find('h4')\n",
    "\n",
    "                if title_element:\n",
    "                    title = title_element.a.text.strip()\n",
    "                    link = title_element.a['href']\n",
    "                else:\n",
    "                    title = \"nana\"\n",
    "                    link = \"nana\"\n",
    "\n",
    "                text_element = article.find('div', class_='newsnippet')\n",
    "                try: \n",
    "                    if text_element:\n",
    "                        text = text_element.p.text.strip() #if text_element else \"nana\"\n",
    "                    else:\n",
    "                        text = \"nana\"\n",
    "                except: \n",
    "                    pass  \n",
    "                \n",
    "                date_element = article.find('div', class_='col-md-8')\n",
    "                if date_element:\n",
    "                    date = date_element.span.text.strip() #if date_element else \"nana\"\n",
    "                else:\n",
    "                    date = \"nana\"\n",
    "\n",
    "                titles.append(title)\n",
    "                links.append(link)\n",
    "                dates.append(date)\n",
    "                texts.append(text)\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "data2 = pd.DataFrame({\"Date\": dates, \"Title\": titles, \"Link\": links,\"Headline\": texts, \"Article\": texts})\n",
    "\n",
    "# Limpieza inicial\n",
    "data2['Article'] = data2['Article'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data2['Article'] = data2['Article'].str.replace(r'\\n', ' ')  \n",
    "data2['Article'] = data2['Article'].str.strip()  \n",
    "data2['Headline'] = data2['Headline'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data2['Headline'] = data2['Headline'].str.replace(r'\\n', ' ')  \n",
    "data2['Headline'] = data2['Headline'].str.strip()  \n",
    "data2['Title'] = data2['Title'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "data2['Title'] = data2['Title'].str.replace(r'\\n', ' ')  \n",
    "data2['Title'] = data2['Title'].str.strip()  \n",
    "\n",
    "data2[\"Source\"] = \"Hydrocarbons\"\n",
    "\n",
    "data2['Date'] = data2['Date'].apply(lambda x: parser.parse(x, fuzzy=True) if pd.notnull(x) else None)\n",
    "\n",
    "#data2.to_csv(\"./Hydrocarbons.csv\", index=False)\n",
    "\n",
    "old_data2 = pd.read_csv(\"./Datos/EC Hydrocarbons.csv\")\n",
    "new_data2 = pd.concat([old_data2,data2], ignore_index=True).drop_duplicates(subset=[\"Link\"])\n",
    "#new_data2[\"Date\"] = pd.to_datetime(new_data2[\"Date\"])\n",
    "#new_data2 = new_data2.sort_values(by=\"Date\")\n",
    "new_data2.to_csv(\"./Datos/EC Hydrocarbons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portafolio \n",
    "#pagsrange = [str(numero) for numero in range(1, 10)] \n",
    "#\n",
    "#titles = []\n",
    "#links = []\n",
    "#dates = []\n",
    "#texts = []\n",
    "#leads = []\n",
    "#\n",
    "#for pag in pagsrange:\n",
    "#    url = \"https://www.data3.co/buscar?q=ecopetrol&page=\" + pag\n",
    "#    try:\n",
    "#        req = Request(url=url)\n",
    "#        response = urlopen(req)\n",
    "#        html = BeautifulSoup(response, \"html.parser\")\n",
    "#        articles = html.find_all(\"div\", class_=\"listing\")\n",
    "#        for article in articles:\n",
    "#            try: \n",
    "#                title = article.find(\"h3\").text.strip() if article else \"nana\"\n",
    "#                link = \"https://www.data3.co\" + article.a['href'] if article else \"nana\"\n",
    "#                lead = article.find('div',class_='listing-epigraph').text.strip() if article else \"nana\"\n",
    "#                lead = lead.replace('\\n', ' ')\n",
    "#                titles.append(title)\n",
    "#                links.append(link)\n",
    "#                leads.append(lead)\n",
    "#                time.sleep(1)\n",
    "#            except:\n",
    "#                titles.append(\"nana\")\n",
    "#                links.append(\"nana\")\n",
    "#                leads.append(\"nana\")\n",
    "#                \n",
    "#    except HTTPError as e:\n",
    "#        if e.code == 404:\n",
    "#            continue\n",
    "#        else:\n",
    "#            continue\n",
    "#\n",
    "#for l in links:\n",
    "#    req = Request(url=l)\n",
    "#    try:\n",
    "#        response = urlopen(req)\n",
    "#        html = BeautifulSoup(response, \"html.parser\")\n",
    "#        try: \n",
    "#\n",
    "#            new = html.find(\"div\", class_=\"article-content\")\n",
    "#            # Eliminar los textos relacionados a los enlaces\n",
    "#            for link in new.find_all('a'):\n",
    "#                link.extract()\n",
    "#            new = new.text.replace('\\n', ' ') if new else \"nana\"\n",
    "#\n",
    "#            text = lead + new\n",
    "#\n",
    "#            date = html.find(\"div\", class_=\"cat-fecha\")\n",
    "#            date = date.find(\"p\", class_='date-time').text if date else \"nana\"\n",
    "#            \n",
    "#\n",
    "#            dates.append(date)\n",
    "#            texts.append(text)\n",
    "#            #leads.append(lead)\n",
    "#            time.sleep(2)\n",
    "#        except:\n",
    "#            dates.append(\"nana\")\n",
    "#            texts.append(\"nana\")\n",
    "#    except HTTPError as e:\n",
    "#        if e.code == 404:\n",
    "#            continue\n",
    "#        else:\n",
    "#            continue   \n",
    "#    #time.sleep(1)\n",
    "#\n",
    "#data3 = pd.DataFrame({\"Date\": dates, \"Title\": titles, \"Link\": links, \"Headline\": leads, \"Article\": texts})\n",
    "#\n",
    "#data3[\"Article\"] = data3[\"Headline\"] + \" \" + data3[\"Article\"]\n",
    "#\n",
    "## Limpieza inicial\n",
    "#data3['Article'] = data3['Article'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "#data3['Article'] = data3['Article'].str.replace(r'\\n', ' ')  \n",
    "#data3['Article'] = data3['Article'].str.strip()  \n",
    "#data3['Headline'] = data3['Headline'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "#data3['Headline'] = data3['Headline'].str.replace(r'\\n', ' ')  \n",
    "#data3['Headline'] = data3['Headline'].str.strip()  \n",
    "#data3['Title'] = data3['Title'].str.replace(r'\\[[^\\]]*\\]', '') \n",
    "#data3['Title'] = data3['Title'].str.replace(r'\\n', ' ')  \n",
    "#data3['Title'] = data3['Title'].str.strip()  \n",
    "#\n",
    "#data3[\"Source\"] = \"Portafolio\"\n",
    "#\n",
    "#data3[\"Date\"] = data3[\"Date\"].str.split(' - ').str[0]\n",
    "#data3[\"Date\"] = data3[\"Date\"].apply(lambda x: ' '.join([meses_mapping[mes] if mes in meses_mapping else mes for mes in x.split()]))\n",
    "#data3[\"Date\"] = pd.to_datetime(data3[\"Date\"], format='%d %b %Y', errors=\"ignore\")\n",
    "#\n",
    "#def convert_date(date):\n",
    "#    try:\n",
    "#        date_object = datetime.strptime(date, '%d %b %Y')\n",
    "#        return date_object.strftime('%Y-%m-%d')\n",
    "#    except ValueError:\n",
    "#        return date\n",
    "#\n",
    "## Aplicar la función a la columna 'Date'\n",
    "#data3['Date'] = data3['Date'].apply(convert_date)\n",
    "#\n",
    "##data3.to_csv(\"./Portafolio.csv\", index=False)\n",
    "#\n",
    "#old_data3 = pd.read_csv(\"./Datos/EC Portafolio.csv\")\n",
    "#old_data3['Date'] = old_data3['Date'].apply(convert_date)\n",
    "#\n",
    "#old_data3 = old_data3[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "#new_data3 = pd.concat([old_data3,data3], ignore_index=True).drop_duplicates(subset=[\"Link\"])\n",
    "#new_data3 = new_data3[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "#new_data3.to_csv(\"./Datos/EC Portafolio.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining dataframes\n",
    "old_final_data = pd.read_csv(\"./Datos/Datos_analisis_EC_traducidos.csv\")\n",
    "old_final_data = old_final_data[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "converted_dates = pd.to_datetime(old_final_data[\"Date\"], format='%Y-%m-%d', errors='coerce')\n",
    "mask = converted_dates.notnull()\n",
    "old_final_data.loc[mask, \"Date\"] = converted_dates[mask]\n",
    "old_final_data = old_final_data[(old_final_data != 'na') & (old_final_data != 'nana')].dropna()\n",
    "new_final_data = pd.concat([old_final_data,new_data1,new_data2], ignore_index=True).drop_duplicates(subset=[\"Link\"])\n",
    "new_final_data = new_final_data[[\"Date\",\"Title\",\"Link\",\"Headline\",\"Article\",\"Source\"]]\n",
    "new_final_data= new_final_data[(new_final_data != 'na') & (new_final_data != 'nana')].dropna()\n",
    "converted_dates = pd.to_datetime(new_final_data[\"Date\"], format='%Y-%m-%d', errors='coerce')\n",
    "mask = converted_dates.notnull()\n",
    "new_final_data.loc[mask, \"Date\"] = converted_dates[mask]\n",
    "new_final_data.to_csv(\"./Datos/Datos_analisis_EC_traducidos.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
